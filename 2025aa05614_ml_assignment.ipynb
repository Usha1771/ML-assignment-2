{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# XGBoost \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Visualization style\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "## Exploratory Data Analysis (EDA)\n",
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q1: Dataset Examination\n",
      "============================================================\n",
      "\n",
      "1. Dataset Size:\n",
      "   Total rows: 1599  Total columns: 12\n",
      "\n",
      "2. Missing Values:\n",
      "    No missing values found\n",
      "\n",
      "3. Feature Names:\n",
      "    Features: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
      "    Target column: 'quality'\n",
      "\n",
      "4. First few rows of dataset:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "\n",
      "Preprocessing complete:\n",
      "Training samples: 1279\n",
      "Test samples: 320\n",
      "Features: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ==============================\n",
    "# Load Wine Quality dataset (red wine)\n",
    "# ==============================\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# ==============================\n",
    "# Q1: Dataset size, missing values, and feature types\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"Q1: Dataset Examination\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Dataset Size\n",
    "print(\"\\n1. Dataset Size:\")\n",
    "print(f\"   Total rows: {data.shape[0]}  Total columns: {data.shape[1]}\")\n",
    "\n",
    "# 2. Missing Values\n",
    "print(\"\\n2. Missing Values:\")\n",
    "missing_values = data.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"    No missing values found\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "# 3. Feature Names (instead of float64 types)\n",
    "print(\"\\n3. Feature Names:\")\n",
    "feature_names = list(data.drop(\"quality\", axis=1).columns)\n",
    "print(\"    Features:\", feature_names)\n",
    "print(\"    Target column: 'quality'\")\n",
    "\n",
    "# 4. First few rows of data\n",
    "print(\"\\n4. First few rows of dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# ==============================\n",
    "# Preprocessing: Features, Target, Split, Scale\n",
    "# ==============================\n",
    "X = data.drop('quality', axis=1)   # Features\n",
    "y = data['quality']                # Target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nPreprocessing complete:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Regression Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Logistic Regression Model\n",
      "============================================================\n",
      "\n",
      "Training set: 1023 samples\n",
      "Validation set: 256 samples\n",
      "Features: 11\n",
      "\n",
      "============================================================\n",
      "Logistic Regression Results:\n",
      "============================================================\n",
      "Training Accuracy : 0.6109\n",
      "Validation Accuracy: 0.5820\n",
      "Validation AUC     : 0.8012\n",
      "Validation Precision: 0.5651\n",
      "Validation Recall   : 0.5820\n",
      "Validation F1       : 0.5670\n",
      "Validation MCC      : 0.3255\n",
      "\n",
      "Logistic Regression model trained and evaluated\n",
      "                 Model  Accuracy       AUC  Precision    Recall  F1 Score  \\\n",
      "0  Logistic Regression  0.582031  0.801206   0.565149  0.582031  0.567013   \n",
      "\n",
      "        MCC  \n",
      "0  0.325498  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# Logistic Regression Model\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"Logistic Regression Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Features: {X_train_split.shape[1]}\")\n",
    "\n",
    "# Build Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = log_reg.predict(X_train_split)\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "y_val_proba = log_reg.predict_proba(X_val)\n",
    "\n",
    "# Metrics\n",
    "train_acc = accuracy_score(y_train_split, y_train_pred)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_auc = roc_auc_score(y_val, y_val_proba, multi_class='ovr')\n",
    "val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "val_mcc = matthews_corrcoef(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training Accuracy : {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation AUC     : {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall   : {val_recall:.4f}\")\n",
    "print(f\"Validation F1       : {val_f1:.4f}\")\n",
    "print(f\"Validation MCC      : {val_mcc:.4f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "results_log_reg = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression'],\n",
    "    'Accuracy': [val_acc],\n",
    "    'AUC': [val_auc],\n",
    "    'Precision': [val_precision],\n",
    "    'Recall': [val_recall],\n",
    "    'F1 Score': [val_f1],\n",
    "    'MCC': [val_mcc]\n",
    "})\n",
    "\n",
    "print(\"\\nLogistic Regression model trained and evaluated\")\n",
    "print(results_log_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Desicion Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Decision Tree Classifier\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Decision Tree Results:\n",
      "============================================================\n",
      "Accuracy   : 0.6094\n",
      "AUC        : 0.6584\n",
      "Precision  : 0.6121\n",
      "Recall     : 0.6094\n",
      "F1 Score   : 0.6095\n",
      "MCC        : 0.3982\n",
      "\n",
      "Decision Tree model trained and evaluated\n",
      "           Model  Accuracy       AUC  Precision    Recall  F1 Score       MCC\n",
      "0  Decision Tree  0.609375  0.658352   0.612092  0.609375  0.609477  0.398241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# Decision Tree Classifier\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"Decision Tree Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build and train model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_dt = dt_model.predict(X_test)\n",
    "y_test_proba_dt = dt_model.predict_proba(X_test)\n",
    "\n",
    "# Metrics (evaluated on test data)\n",
    "acc_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "auc_dt = roc_auc_score(y_test, y_test_proba_dt, multi_class='ovr')\n",
    "precision_dt = precision_score(y_test, y_test_pred_dt, average='weighted')\n",
    "recall_dt = recall_score(y_test, y_test_pred_dt, average='weighted')\n",
    "f1_dt = f1_score(y_test, y_test_pred_dt, average='weighted')\n",
    "mcc_dt = matthews_corrcoef(y_test, y_test_pred_dt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Decision Tree Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy   : {acc_dt:.4f}\")\n",
    "print(f\"AUC        : {auc_dt:.4f}\")\n",
    "print(f\"Precision  : {precision_dt:.4f}\")\n",
    "print(f\"Recall     : {recall_dt:.4f}\")\n",
    "print(f\"F1 Score   : {f1_dt:.4f}\")\n",
    "print(f\"MCC        : {mcc_dt:.4f}\")\n",
    "\n",
    "# Store results (standardized schema)\n",
    "results_dt = pd.DataFrame({\n",
    "    'Model': ['Decision Tree'],\n",
    "    'Accuracy': [acc_dt],\n",
    "    'AUC': [auc_dt],\n",
    "    'Precision': [precision_dt],\n",
    "    'Recall': [recall_dt],\n",
    "    'F1 Score': [f1_dt],\n",
    "    'MCC': [mcc_dt]\n",
    "})\n",
    "\n",
    "print(\"\\nDecision Tree model trained and evaluated\")\n",
    "print(results_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## K-Nearest Neighbor Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KNN Classifier\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "KNN Results:\n",
      "============================================================\n",
      "Accuracy   : 0.6094\n",
      "AUC        : 0.6983\n",
      "Precision  : 0.5841\n",
      "Recall     : 0.6094\n",
      "F1 Score   : 0.5959\n",
      "MCC        : 0.3733\n",
      "\n",
      "KNN model trained and evaluated\n",
      "  Model  Accuracy       AUC  Precision    Recall  F1 Score       MCC\n",
      "0   KNN  0.609375  0.698329   0.584116  0.609375  0.595887  0.373313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# KNN Classifier\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"KNN Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build and train model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_knn = knn_model.predict(X_test)\n",
    "y_test_proba_knn = knn_model.predict_proba(X_test)\n",
    "\n",
    "# Metrics (evaluated on test data)\n",
    "acc_knn = accuracy_score(y_test, y_test_pred_knn)\n",
    "auc_knn = roc_auc_score(y_test, y_test_proba_knn, multi_class='ovr')\n",
    "precision_knn = precision_score(y_test, y_test_pred_knn, average='weighted')\n",
    "recall_knn = recall_score(y_test, y_test_pred_knn, average='weighted')\n",
    "f1_knn = f1_score(y_test, y_test_pred_knn, average='weighted')\n",
    "mcc_knn = matthews_corrcoef(y_test, y_test_pred_knn)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KNN Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy   : {acc_knn:.4f}\")\n",
    "print(f\"AUC        : {auc_knn:.4f}\")\n",
    "print(f\"Precision  : {precision_knn:.4f}\")\n",
    "print(f\"Recall     : {recall_knn:.4f}\")\n",
    "print(f\"F1 Score   : {f1_knn:.4f}\")\n",
    "print(f\"MCC        : {mcc_knn:.4f}\")\n",
    "\n",
    "# Store results (standardized schema)\n",
    "results_knn = pd.DataFrame({\n",
    "    'Model': ['KNN'],\n",
    "    'Accuracy': [acc_knn],\n",
    "    'AUC': [auc_knn],\n",
    "    'Precision': [precision_knn],\n",
    "    'Recall': [recall_knn],\n",
    "    'F1 Score': [f1_knn],\n",
    "    'MCC': [mcc_knn]\n",
    "})\n",
    "\n",
    "print(\"\\nKNN model trained and evaluated\")\n",
    "print(results_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Naive Bayes Classifier - Gaussian or Multinomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Naive Bayes Classifier\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Naive Bayes Results:\n",
      "============================================================\n",
      "Accuracy   : 0.5625\n",
      "AUC        : 0.6838\n",
      "Precision  : 0.5745\n",
      "Recall     : 0.5625\n",
      "F1 Score   : 0.5681\n",
      "MCC        : 0.3299\n",
      "\n",
      "Naive Bayes model trained and evaluated\n",
      "                      Model  Accuracy       AUC  Precision  Recall  F1 Score  \\\n",
      "0  Naive Bayes (GaussianNB)    0.5625  0.683783   0.574461  0.5625  0.568067   \n",
      "\n",
      "        MCC  \n",
      "0  0.329911  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# Naive Bayes Classifier\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"Naive Bayes Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build and train model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_nb = nb_model.predict(X_test)\n",
    "y_test_proba_nb = nb_model.predict_proba(X_test)\n",
    "\n",
    "# Metrics (evaluated on test data)\n",
    "acc_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "auc_nb = roc_auc_score(y_test, y_test_proba_nb, multi_class='ovr')\n",
    "precision_nb = precision_score(y_test, y_test_pred_nb, average='weighted')\n",
    "recall_nb = recall_score(y_test, y_test_pred_nb, average='weighted')\n",
    "f1_nb = f1_score(y_test, y_test_pred_nb, average='weighted')\n",
    "mcc_nb = matthews_corrcoef(y_test, y_test_pred_nb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy   : {acc_nb:.4f}\")\n",
    "print(f\"AUC        : {auc_nb:.4f}\")\n",
    "print(f\"Precision  : {precision_nb:.4f}\")\n",
    "print(f\"Recall     : {recall_nb:.4f}\")\n",
    "print(f\"F1 Score   : {f1_nb:.4f}\")\n",
    "print(f\"MCC        : {mcc_nb:.4f}\")\n",
    "\n",
    "# Store results (standardized schema)\n",
    "results_nb = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes (GaussianNB)'],\n",
    "    'Accuracy': [acc_nb],\n",
    "    'AUC': [auc_nb],\n",
    "    'Precision': [precision_nb],\n",
    "    'Recall': [recall_nb],\n",
    "    'F1 Score': [f1_nb],\n",
    "    'MCC': [mcc_nb]\n",
    "})\n",
    "\n",
    "print(\"\\nNaive Bayes model trained and evaluated\")\n",
    "print(results_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ensemble Model - Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Random Forest Classifier\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Random Forest Results:\n",
      "============================================================\n",
      "Accuracy   : 0.6750\n",
      "AUC        : 0.7907\n",
      "Precision  : 0.6539\n",
      "Recall     : 0.6750\n",
      "F1 Score   : 0.6599\n",
      "MCC        : 0.4746\n",
      "\n",
      "Random Forest model trained and evaluated\n",
      "           Model  Accuracy       AUC  Precision  Recall  F1 Score       MCC\n",
      "0  Random Forest     0.675  0.790705   0.653858   0.675  0.659933  0.474554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# Random Forest Classifier\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"Random Forest Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build and train model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Metrics (evaluated on test data)\n",
    "acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "auc_rf = roc_auc_score(y_test, y_test_proba_rf, multi_class='ovr')\n",
    "precision_rf = precision_score(y_test, y_test_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_test_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_test_pred_rf, average='weighted')\n",
    "mcc_rf = matthews_corrcoef(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy   : {acc_rf:.4f}\")\n",
    "print(f\"AUC        : {auc_rf:.4f}\")\n",
    "print(f\"Precision  : {precision_rf:.4f}\")\n",
    "print(f\"Recall     : {recall_rf:.4f}\")\n",
    "print(f\"F1 Score   : {f1_rf:.4f}\")\n",
    "print(f\"MCC        : {mcc_rf:.4f}\")\n",
    "\n",
    "# Store results (standardized schema)\n",
    "results_rf = pd.DataFrame({\n",
    "    'Model': ['Random Forest'],\n",
    "    'Accuracy': [acc_rf],\n",
    "    'AUC': [auc_rf],\n",
    "    'Precision': [precision_rf],\n",
    "    'Recall': [recall_rf],\n",
    "    'F1 Score': [f1_rf],\n",
    "    'MCC': [mcc_rf]\n",
    "})\n",
    "\n",
    "print(\"\\nRandom Forest model trained and evaluated\")\n",
    "print(results_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ensemble Model - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "XGBoost Classifier\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "XGBoost Results:\n",
      "============================================================\n",
      "Accuracy   : 0.6594\n",
      "AUC        : 0.8374\n",
      "Precision  : 0.6520\n",
      "Recall     : 0.6594\n",
      "F1 Score   : 0.6488\n",
      "MCC        : 0.4535\n",
      "\n",
      "XGBoost model trained and evaluated\n",
      "     Model  Accuracy       AUC  Precision    Recall  F1 Score       MCC\n",
      "0  XGBoost  0.659375  0.837422   0.651973  0.659375  0.648799  0.453534\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# XGBoost Classifier\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"XGBoost Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Encode target labels to start from 0\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Build and train model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train_enc)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "y_test_proba_xgb = xgb_model.predict_proba(X_test)\n",
    "\n",
    "# Metrics (evaluated on test data)\n",
    "acc_xgb = accuracy_score(y_test_enc, y_test_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test_enc, y_test_proba_xgb, multi_class='ovr')\n",
    "precision_xgb = precision_score(y_test_enc, y_test_pred_xgb, average='weighted')\n",
    "recall_xgb = recall_score(y_test_enc, y_test_pred_xgb, average='weighted')\n",
    "f1_xgb = f1_score(y_test_enc, y_test_pred_xgb, average='weighted')\n",
    "mcc_xgb = matthews_corrcoef(y_test_enc, y_test_pred_xgb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBoost Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy   : {acc_xgb:.4f}\")\n",
    "print(f\"AUC        : {auc_xgb:.4f}\")\n",
    "print(f\"Precision  : {precision_xgb:.4f}\")\n",
    "print(f\"Recall     : {recall_xgb:.4f}\")\n",
    "print(f\"F1 Score   : {f1_xgb:.4f}\")\n",
    "print(f\"MCC        : {mcc_xgb:.4f}\")\n",
    "\n",
    "# Store results (standardized schema)\n",
    "results_xgb = pd.DataFrame({\n",
    "    'Model': ['XGBoost'],\n",
    "    'Accuracy': [acc_xgb],\n",
    "    'AUC': [auc_xgb],\n",
    "    'Precision': [precision_xgb],\n",
    "    'Recall': [recall_xgb],\n",
    "    'F1 Score': [f1_xgb],\n",
    "    'MCC': [mcc_xgb]\n",
    "})\n",
    "\n",
    "print(\"\\nXGBoost model trained and evaluated\")\n",
    "print(results_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparision Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Final Comparison Table\n",
      "============================================================\n",
      "\n",
      "Comparison of all models:\n",
      "                   Model  Accuracy      AUC  Precision   Recall  F1 Score      MCC\n",
      "     Logistic Regression  0.582031 0.801206   0.565149 0.582031  0.567013 0.325498\n",
      "           Decision Tree  0.609375 0.658352   0.612092 0.609375  0.609477 0.398241\n",
      "                     KNN  0.609375 0.698329   0.584116 0.609375  0.595887 0.373313\n",
      "Naive Bayes (GaussianNB)  0.562500 0.683783   0.574461 0.562500  0.568067 0.329911\n",
      "           Random Forest  0.675000 0.790705   0.653858 0.675000  0.659933 0.474554\n",
      "                 XGBoost  0.659375 0.837422   0.651973 0.659375  0.648799 0.453534\n",
      "\n",
      "Final comparison table created and saved as 'results_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Final Comparison Table\n",
    "# ==============================\n",
    "print(\"=\" * 60)\n",
    "print(\"Final Comparison Table\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Concatenate all model results\n",
    "comparison = pd.concat([\n",
    "    results_log_reg,\n",
    "    results_dt,\n",
    "    results_knn,\n",
    "    results_nb,\n",
    "    results_rf,\n",
    "    results_xgb\n",
    "], ignore_index=True)\n",
    "\n",
    "# Print full table in one block\n",
    "print(\"\\nComparison of all models:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison.to_csv(\"results_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nFinal comparison table created and saved as 'results_summary.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
